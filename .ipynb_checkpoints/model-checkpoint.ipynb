{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from itertools import product\n",
    "\n",
    "conn = sql.connect(\"states.db\")\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables_df = pd.read_sql(tables_query, conn)\n",
    "table_names = tables_df['name'].tolist()[1:]\n",
    "\n",
    "\n",
    "dataframes = {}\n",
    "for table_name in table_names:\n",
    "    dataframes[table_name] = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_rate = dataframes['unemployment_rate']\n",
    "state_survival_rates = dataframes['survival_rates']\n",
    "industry_survival_rates = dataframes['industry_survival_rates']\n",
    "establishments = dataframes['establishments'] # use establishments data, has count. Tells how many industry firms are in a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year                                       Industry State Establishments\n",
      "0  1986                                   Construction    AL           6413\n",
      "1  1986     Agriculture, Forestry, Fishing and Hunting    AL            838\n",
      "2  1986  Mining, Quarrying, and Oil and Gas Extraction    AL            318\n",
      "3  1986                                Wholesale Trade    AL           6158\n",
      "4  1986                 Transportation and Warehousing    AL           3126\n"
     ]
    }
   ],
   "source": [
    "print(establishments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reformatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into 1530 samples for state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\1960791451.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_survival_rates_filtered[\"Year\"] = state_survival_rates_filtered[\"Year\"].astype('int')\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\1960791451.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\1960791451.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_survival_rates_filtered['Surviving Establishments'] = state_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Year Established  Year  Surviving Establishments  \\\n",
      "1             1994  1995                      7275   \n",
      "2             1994  1996                      6293   \n",
      "3             1994  1997                      5630   \n",
      "4             1994  1998                      5137   \n",
      "5             1994  1999                      4664   \n",
      "\n",
      "  Total Employment of Survivors Survival Rates Since Birth  \\\n",
      "1                        65,993                       78.6   \n",
      "2                        63,045                       68.0   \n",
      "3                        60,887                       60.8   \n",
      "4                        59,875                       55.5   \n",
      "5                        56,608                       50.4   \n",
      "\n",
      "   Survival Rates of Previous Year's Survivors  \\\n",
      "1                                         78.6   \n",
      "2                                         86.5   \n",
      "3                                         89.5   \n",
      "4                                         91.2   \n",
      "5                                         90.8   \n",
      "\n",
      "  Average Employment of Survivors State  \n",
      "1                             9.1    AL  \n",
      "2                            10.0    AL  \n",
      "3                            10.8    AL  \n",
      "4                            11.7    AL  \n",
      "5                            12.1    AL  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\1960791451.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  state_survival_rates_grouped = state_survival_rates_filtered.groupby([\"Year\", \"State\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))\n"
     ]
    }
   ],
   "source": [
    "state_survival_rates_filtered = state_survival_rates[state_survival_rates[\"Year Established\"] != state_survival_rates[\"Year\"]]\n",
    "state_survival_rates_filtered[\"Year\"] = state_survival_rates_filtered[\"Year\"].astype('int')\n",
    "state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
    "state_survival_rates_filtered['Surviving Establishments'] = state_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n",
    "state_survival_rates_grouped = state_survival_rates_filtered.groupby([\"Year\", \"State\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))\n",
    "\n",
    "print(state_survival_rates_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get reformatted industry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\2188420590.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_survival_rates_filtered[\"Year\"] = industry_survival_rates_filtered[\"Year\"].astype('int')\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\2188420590.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\2188420590.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_survival_rates_filtered['Surviving Establishments'] = industry_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Year Established  Year  Surviving Establishments  \\\n",
      "1             1994  1995                      5537   \n",
      "2             1994  1996                      4826   \n",
      "3             1994  1997                      4406   \n",
      "4             1994  1998                      4005   \n",
      "5             1994  1999                      3729   \n",
      "\n",
      "  Total Employment of Survivors Survival Rates Since Birth  \\\n",
      "1                        51,066                       82.1   \n",
      "2                        48,263                       71.6   \n",
      "3                        47,634                       65.4   \n",
      "4                        45,444                       59.4   \n",
      "5                        39,377                       55.3   \n",
      "\n",
      "   Survival Rates of Previous Year's Survivors  \\\n",
      "1                                         82.1   \n",
      "2                                         87.2   \n",
      "3                                         91.3   \n",
      "4                                         90.9   \n",
      "5                                         93.1   \n",
      "\n",
      "  Average Employment of Survivors                                    Industry  \n",
      "1                             9.2  Agriculture, Forestry, Fishing and Hunting  \n",
      "2                            10.0  Agriculture, Forestry, Fishing and Hunting  \n",
      "3                            10.8  Agriculture, Forestry, Fishing and Hunting  \n",
      "4                            11.3  Agriculture, Forestry, Fishing and Hunting  \n",
      "5                            10.6  Agriculture, Forestry, Fishing and Hunting  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\2188420590.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  industry_survival_rates_grouped = industry_survival_rates_filtered.groupby([\"Year\", \"Industry\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))\n"
     ]
    }
   ],
   "source": [
    "industry_survival_rates_filtered = industry_survival_rates[industry_survival_rates[\"Year Established\"] != industry_survival_rates[\"Year\"]]\n",
    "industry_survival_rates_filtered[\"Year\"] = industry_survival_rates_filtered[\"Year\"].astype('int')\n",
    "industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
    "industry_survival_rates_filtered['Surviving Establishments'] = industry_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n",
    "industry_survival_rates_grouped = industry_survival_rates_filtered.groupby([\"Year\", \"Industry\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))\n",
    "\n",
    "print(industry_survival_rates_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\3764304121.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  total_survival_rate_grouped = total_survival_rate.groupby([\"Year\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))\n"
     ]
    }
   ],
   "source": [
    "total_survival_rate = pd.concat([state_survival_rates_filtered, industry_survival_rates_filtered])\n",
    "total_survival_rate_grouped = total_survival_rate.groupby([\"Year\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))\n",
    "# ^ overall business survival rate by year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_bayesian_survival_probabilities(state_survival_rates_filtered, industry_survival_rates_filtered, establishments):\n",
    "    probabilities = []\n",
    "    years = state_survival_rates_filtered['Year'].unique() # loop through the unique years in the list\n",
    "\n",
    "    # P(Survival | State, Sector) = P(Survival) * P(State, Sector | Survival)P(State, Sector)\n",
    "    \n",
    "    for year in years:\n",
    "        df_state = state_survival_rates_filtered[state_survival_rates_filtered['Year'] == year] # data during the current iterated year\n",
    "        df_industry = industry_survival_rates_filtered[industry_survival_rates_filtered['Year'] == year]\n",
    "        df_establishments = establishments[establishments['Year'] == year]\n",
    "\n",
    "        df_establishments['Establishments'] = df_establishments['Establishments'].astype(str).str.replace(',', '').astype(int)\n",
    "        # establishments was originally in string format\n",
    "        \n",
    "        total_establishments = df_establishments['Establishments'].sum()\n",
    "\n",
    "        total_survivors = df_state['Surviving Establishments'].sum()\n",
    "        p_survival = total_survivors / total_establishments if total_establishments > 0 else 0  # P(Survival)\n",
    "        \n",
    "        # Compute P(State, Sector) - probability of being in a state-sector\n",
    "        df_state = df_state.merge(df_establishments, on=['Year', 'State'], how='left')\n",
    "        df_industry = df_industry.merge(df_establishments, on=['Year', 'Industry'], how='left')\n",
    "        \n",
    "        df_state['P_State'] = df_state['Establishments'] / total_establishments\n",
    "        df_industry['P_Sector'] = df_industry['Establishments'] / total_establishments\n",
    "        \n",
    "        # Compute P(State | Survival) and P(Sector | Survival)\n",
    "        total_state_survivors = df_state['Surviving Establishments'].sum()\n",
    "        total_industry_survivors = df_industry['Surviving Establishments'].sum()\n",
    "        \n",
    "        df_state['P_State_given_Survival'] = df_state['Surviving Establishments'] / total_state_survivors\n",
    "        df_industry['P_Sector_given_Survival'] = df_industry['Surviving Establishments'] / total_industry_survivors\n",
    "\n",
    "        df_merged = pd.merge(\n",
    "            df_state[['Year', 'State', 'Industry', 'P_State', 'P_State_given_Survival']],\n",
    "            df_industry[['Year', 'State', 'Industry', 'P_Sector', 'P_Sector_given_Survival']],\n",
    "            on=['Year', 'State', 'Industry'], \n",
    "            how='inner'  # Use inner instead of outer to keep only matching records\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Compute P(Survival | State, Sector) using Bayesian formula\n",
    "        df_merged['P_Survival_given_State_Sector'] = (\n",
    "            p_survival * df_merged['P_State_given_Survival'] * df_merged['P_Sector_given_Survival'] /\n",
    "            (df_merged['P_State'] * df_merged['P_Sector'])\n",
    "        ).fillna(0)  # Handle divide-by-zero cases\n",
    "        \n",
    "        probabilities.append(df_merged[['Year', 'State', 'Industry', 'P_Survival_given_State_Sector']])\n",
    "    \n",
    "    return pd.concat(probabilities, ignore_index=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\874857580.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_establishments['Establishments'] = df_establishments['Establishments'].astype(str).str.replace(',', '').astype(int)\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\874857580.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_establishments['Establishments'] = df_establishments['Establishments'].astype(str).str.replace(',', '').astype(int)\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\874857580.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_establishments['Establishments'] = df_establishments['Establishments'].astype(str).str.replace(',', '').astype(int)\n",
      "C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_31256\\874857580.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_establishments['Establishments'] = df_establishments['Establishments'].astype(str).str.replace(',', '').astype(int)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 265. MiB for an array with shape (2, 17395488) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bayesian_survival_probabilities \u001b[38;5;241m=\u001b[39m compute_bayesian_survival_probabilities(state_survival_rates_filtered, industry_survival_rates_filtered, establishments)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(bayesian_survival_probabilities\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[110], line 66\u001b[0m, in \u001b[0;36mcompute_bayesian_survival_probabilities\u001b[1;34m(state_survival_rates_filtered, industry_survival_rates_filtered, establishments)\u001b[0m\n\u001b[0;32m     61\u001b[0m all_combinations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(product([year], all_states, all_industries)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndustry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Merge the probability data onto the full grid\u001b[39;00m\n\u001b[0;32m     64\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m all_combinations \\\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;241m.\u001b[39mmerge(df_state[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_State\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_State_given_Survival\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;241m.\u001b[39mmerge(df_industry[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndustry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_Sector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_Sector_given_Survival\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndustry\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Fill NaN values (for missing state-industry combinations)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m df_merged\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10833\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10834\u001b[0m         right,\n\u001b[0;32m  10835\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m  10836\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m  10837\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m  10838\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m  10839\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m  10840\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m  10841\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m  10842\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m  10843\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m  10844\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m  10845\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10846\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    877\u001b[0m left\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m llabels\n\u001b[0;32m    878\u001b[0m right\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m rlabels\n\u001b[1;32m--> 879\u001b[0m result \u001b[38;5;241m=\u001b[39m concat([left, right], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m, deep\u001b[38;5;241m=\u001b[39mdeep)\n\u001b[0;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 265. MiB for an array with shape (2, 17395488) and data type object"
     ]
    }
   ],
   "source": [
    "bayesian_survival_probabilities = compute_bayesian_survival_probabilities(state_survival_rates_filtered, industry_survival_rates_filtered, establishments)\n",
    "print(bayesian_survival_probabilities.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to multiply each combination of state and industry, then divide by that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for (year, state), state_val in state_survival_rates_grouped.items():\n",
    "    for (ind_year, industry), ind_val in industry_survival_rates_grouped.items():\n",
    "        if year == ind_year:\n",
    "            result = (year, state, industry), (state_val * ind_val) / total_survival_rate_grouped[year]\n",
    "            results.append(result)\n",
    "\n",
    "result = pd.Series(dict(results))\n",
    "unemployment_rate[\"Year\"] = unemployment_rate[\"Year\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = state_survival_rates_grouped.unstack()\n",
    "industry_df = industry_survival_rates_grouped.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_state = dict(zip(state_df.columns, range(len(state_df.columns))))\n",
    "numerical_state_rev = dict(zip(range(len(state_df.columns)), state_df.columns))\n",
    "numerical_industry = dict(zip(industry_df.columns, range(len(industry_df.columns))))\n",
    "numerical_industry_rev = dict(zip(range(len(industry_df.columns)), industry_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = []\n",
    "for year in result.index.get_level_values(0).unique():\n",
    "    for state in state_df.columns:\n",
    "        unemployment = unemployment_rate[(unemployment_rate[\"Year\"] == year) & (unemployment_rate[\"State\"] == state)][\"Unemployment Rate\"]\n",
    "        for industry in industry_df.columns:\n",
    "            response = result[year, state, industry]\n",
    "            pre_df.append((year, numerical_state[state], numerical_industry[industry], float(unemployment.iloc[0]), response))\n",
    "final_dataset = pd.DataFrame(pre_df, columns=[\"Year\", \"State\", \"Industry\", \"Unemployment Rate\", \"Response\"])\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EconDataset(Dataset):\n",
    "    def __init__(self, data, state_col, industry_col, unemployment_col, response_col):\n",
    "        self.data = data.copy()\n",
    "        self.state_col = state_col\n",
    "        self.industry_col = industry_col\n",
    "        self.unemployment_col = unemployment_col\n",
    "        self.response_col = response_col\n",
    "        self.state_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.industry_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        self.encoded_states = self.state_encoder.fit_transform(self.data[[self.state_col]]) # Applying one-hot encoding to the state column\n",
    "        self.encoded_industries = self.industry_encoder.fit_transform(self.data[[self.industry_col]]) # Applying one-hot encoding to the industry\n",
    "        self.unemployment_stats = self.data[self.unemployment_col].values.reshape(-1, 1) # Turns into column vector\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state = self.encoded_states[idx]\n",
    "        industry = self.encoded_industries[idx]\n",
    "        unemployment = self.unemployment_stats[idx]\n",
    "        predictor = np.concatenate((unemployment, state, industry), axis=0)\n",
    "        response = self.data[self.response_col].values[idx]\n",
    "        return torch.tensor(predictor, dtype=torch.float32), torch.tensor(response, dtype=torch.float32).view(1) #Response is reshaped to a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurvivalData = EconDataset(final_dataset, \"State\", \"Industry\", \"Unemployment Rate\", \"Response\")\n",
    "dataloader = DataLoader(SurvivalData, batch_size=2, shuffle=True)  # shuffle = True is important for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalRateModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size = 1):\n",
    "        super(SurvivalRateModel, self).__init__()\n",
    "\n",
    "        # Define layers:\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "    def train_step(self, x, y, criterion, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = self(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def train_loop(self, dataloader, num_epochs=10, learning_rate=0.001, device = \"cpu\"):\n",
    "        self.to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.train() #set model to train mode.\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for x, y in dataloader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                loss = self.train_step(x, y, criterion, optimizer)\n",
    "                total_loss += loss\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EconDataset(final_dataset, \"State\", \"Industry\", \"Unemployment Rate\", \"Response\")\n",
    "dataloader = DataLoader(data, batch_size=2, shuffle=True)\n",
    "Model = SurvivalRateModel(input_size=1 + len(data.state_encoder.categories_[0]) + len(data.industry_encoder.categories_[0]), hidden_size1=4, hidden_size2=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 393.2532\n",
      "Epoch [2/10], Loss: 13.7737\n",
      "Epoch [3/10], Loss: 13.7004\n",
      "Epoch [4/10], Loss: 13.6063\n",
      "Epoch [5/10], Loss: 13.4531\n",
      "Epoch [6/10], Loss: 13.3315\n",
      "Epoch [7/10], Loss: 13.1304\n",
      "Epoch [8/10], Loss: 13.0063\n",
      "Epoch [9/10], Loss: 12.7960\n",
      "Epoch [10/10], Loss: 12.6916\n"
     ]
    }
   ],
   "source": [
    "Model.train_loop(dataloader, num_epochs=10, learning_rate=0.001, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
