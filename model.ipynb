{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "conn = sql.connect(\"states.db\")\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables_df = pd.read_sql(tables_query, conn)\n",
    "table_names = tables_df['name'].tolist()[1:]\n",
    "\n",
    "\n",
    "dataframes = {}\n",
    "for table_name in table_names:\n",
    "    dataframes[table_name] = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_rate = dataframes['unemployment_rate']\n",
    "state_survival_rates = dataframes['survival_rates']\n",
    "industry_survival_rates = dataframes['industry_survival_rates']\n",
    "establishments = dataframes['establishments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reformatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into 1530 samples for state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_47581/1002942169.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_survival_rates_filtered[\"Year\"] = state_survival_rates_filtered[\"Year\"].astype('int')\n",
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_47581/1002942169.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_47581/1002942169.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_survival_rates_filtered['Surviving Establishments'] = state_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "state_survival_rates_filtered = state_survival_rates[state_survival_rates[\"Year Established\"] != state_survival_rates[\"Year\"]]\n",
    "state_survival_rates_filtered[\"Year\"] = state_survival_rates_filtered[\"Year\"].astype('int')\n",
    "state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = state_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
    "state_survival_rates_filtered['Surviving Establishments'] = state_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n",
    "state_survival_rates_grouped = state_survival_rates_filtered.groupby([\"Year\", \"State\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get reformatted industry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_47581/1975599014.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_survival_rates_filtered[\"Year\"] = industry_survival_rates_filtered[\"Year\"].astype('int')\n",
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_47581/1975599014.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
      "/var/folders/m7/6v1zwk1505d2ph7h2jch6t340000gn/T/ipykernel_47581/1975599014.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  industry_survival_rates_filtered['Surviving Establishments'] = industry_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "industry_survival_rates_filtered = industry_survival_rates[industry_survival_rates[\"Year Established\"] != industry_survival_rates[\"Year\"]]\n",
    "industry_survival_rates_filtered[\"Year\"] = industry_survival_rates_filtered[\"Year\"].astype('int')\n",
    "industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"] = industry_survival_rates_filtered[\"Survival Rates of Previous Year's Survivors\"].astype('float')\n",
    "industry_survival_rates_filtered['Surviving Establishments'] = industry_survival_rates_filtered['Surviving Establishments'].str.replace(',', '').astype(int)\n",
    "industry_survival_rates_grouped = industry_survival_rates_filtered.groupby([\"Year\", \"Industry\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_survival_rate = pd.concat([state_survival_rates_filtered, industry_survival_rates_filtered])\n",
    "total_survival_rate_grouped = total_survival_rate.groupby([\"Year\"]).apply(lambda x: np.average(x['Survival Rates of Previous Year\\'s Survivors'], weights=x['Surviving Establishments']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to multiply each combination of state and industry, then divide by that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for (year, state), state_val in state_survival_rates_grouped.items():\n",
    "    for (ind_year, industry), ind_val in industry_survival_rates_grouped.items():\n",
    "        if year == ind_year:\n",
    "            result = (year, state, industry), (state_val * ind_val) / total_survival_rate_grouped[year]\n",
    "            results.append(result)\n",
    "\n",
    "result = pd.Series(dict(results))\n",
    "unemployment_rate[\"Year\"] = unemployment_rate[\"Year\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = state_survival_rates_grouped.unstack()\n",
    "industry_df = industry_survival_rates_grouped.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_state = dict(zip(state_df.columns, range(len(state_df.columns))))\n",
    "numerical_state_rev = dict(zip(range(len(state_df.columns)), state_df.columns))\n",
    "numerical_industry = dict(zip(industry_df.columns, range(len(industry_df.columns))))\n",
    "numerical_industry_rev = dict(zip(range(len(industry_df.columns)), industry_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = []\n",
    "for year in result.index.get_level_values(0).unique():\n",
    "    for state in state_df.columns:\n",
    "        unemployment = unemployment_rate[(unemployment_rate[\"Year\"] == year) & (unemployment_rate[\"State\"] == state)][\"Unemployment Rate\"]\n",
    "        for industry in industry_df.columns:\n",
    "            response = result[year, state, industry]\n",
    "            pre_df.append((year, numerical_state[state], numerical_industry[industry], float(unemployment.iloc[0]), response))\n",
    "final_dataset = pd.DataFrame(pre_df, columns=[\"Year\", \"State\", \"Industry\", \"Unemployment Rate\", \"Response\"])\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EconDataset(Dataset):\n",
    "    def __init__(self, data, state_col, industry_col, unemployment_col, response_col):\n",
    "        self.data = data.copy()\n",
    "        self.state_col = state_col\n",
    "        self.industry_col = industry_col\n",
    "        self.unemployment_col = unemployment_col\n",
    "        self.response_col = response_col\n",
    "        self.state_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.industry_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        self.encoded_states = self.state_encoder.fit_transform(self.data[[self.state_col]]) # Applying one-hot encoding to the state column\n",
    "        self.encoded_industries = self.industry_encoder.fit_transform(self.data[[self.industry_col]]) # Applying one-hot encoding to the industry\n",
    "        self.unemployment_stats = self.data[self.unemployment_col].values.reshape(-1, 1) # Turns into column vector\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state = self.encoded_states[idx]\n",
    "        industry = self.encoded_industries[idx]\n",
    "        unemployment = self.unemployment_stats[idx]\n",
    "        predictor = np.concatenate((unemployment, state, industry), axis=0)\n",
    "        response = self.data[self.response_col].values[idx]\n",
    "        return torch.tensor(predictor, dtype=torch.float32), torch.tensor(response, dtype=torch.float32).view(1) #Response is reshaped to a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurvivalData = CustomDataset(final_dataset, \"State\", \"Industry\", \"Unemployment Rate\", \"Response\")\n",
    "dataloader = DataLoader(SurvivalData, batch_size=2, shuffle=True)  # shuffle = True is important for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalRateModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size = 1):\n",
    "        super(SurvivalRateModel, self).__init__()\n",
    "\n",
    "        # Define layers:\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "    def train_step(self, x, y, criterion, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = self(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def train_loop(self, dataloader, num_epochs=10, learning_rate=0.001, device = \"cpu\"):\n",
    "        self.to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.train() #set model to train mode.\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for x, y in dataloader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                loss = self.train_step(x, y, criterion, optimizer)\n",
    "                total_loss += loss\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EconDataset(final_dataset, \"State\", \"Industry\", \"Unemployment Rate\", \"Response\")\n",
    "dataloader = DataLoader(data, batch_size=2, shuffle=True)\n",
    "Model = SurvivalRateModel(input_size=1 + len(data.state_encoder.categories_[0]) + len(data.industry_encoder.categories_[0]), hidden_size1=4, hidden_size2=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1980.7699\n",
      "Epoch [2/10], Loss: 16.1663\n",
      "Epoch [3/10], Loss: 16.1649\n",
      "Epoch [4/10], Loss: 16.1720\n",
      "Epoch [5/10], Loss: 16.1659\n",
      "Epoch [6/10], Loss: 16.1555\n",
      "Epoch [7/10], Loss: 16.1643\n",
      "Epoch [8/10], Loss: 16.1726\n",
      "Epoch [9/10], Loss: 16.1633\n",
      "Epoch [10/10], Loss: 16.1603\n"
     ]
    }
   ],
   "source": [
    "Model.train_loop(dataloader, num_epochs=10, learning_rate=0.001, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B-25W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
